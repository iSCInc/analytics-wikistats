#!/bin/sh

# Prepare several csv files, ready for importing into analytics database
# All generated files have _in_ in name signalling these contain data ready for importing into database
# One input record corresponds to one database record

ulimit -v 8000000

log="analytics_generate_csv_files_log.txt" 

# www.walkernews.net/2007/06/03/date-arithmetic-in-linux-shell-scripts
yyyymm=$(date -d "2 month ago" +"%Y-%m")
echo process data up to $yyyymm and write to rc-$yyyymm.zip

clear
cd /a/wikistats/analytics

# AnalyticsPrepBinariesData.pl read counts for binaries which were generated by wikistats 
# and which reside in /a/wikistats/csv_[project code]/StatisticsPerBinariesExtension.csv
# It filters and reorganizes data and produces analytics_in_binaries.csv
# Output csv contains: project code, language, month, extension name, count

perl AnalyticsPrepBinariesData.pl -i /a/wikistats/csv/ -o /a/wikistats/analytics/ | tee $log | cat

# AnalyticsPrepComscoreData.pl scans /a/analytics/comscore for newest comScore csv files (with data for last 14 months) 
# parses those csv files, adds/replaces data from these csv files into master files (containing full history)
# and generates input csv file analytics_in_comscore.csv ready for importing into database
#
# note : these csv files were manually downloaded from http://mymetrix.comscore.com/app/report.aspx 
# and given more descriptive names, script finds newest files based on partial name search 
#
# -r replace (default is add only)
# -i input folder, contains manually downloaded csv files from comScore (or xls files manually converted to csv) 
# -m master files with full history
# -o output csv file, with reach per region, UV's per region and UV's per top web property, ready for import into database

perl AnalyticsPrepComscoreData.pl -r -i /a/wikistats/analytics/comscore -m /a/wikistats/analytics -o /a/wikistats/analytics | tee -a $log | cat

# AnalyticsPrepWikiCountsOutput.pl reads a plethora of fields from several csv files from wikistats process
# It filters and reorganizes data and produces analytics_in_wikistats.csv, ready for import into analytics database 

perl AnalyticsPrepWikiCountsOutputMisc.pl -i /a/wikistats/csv/ -o /a/wikistats/analytics            | tee -a $log | cat
perl AnalyticsPrepWikiCountsOutputCore.pl -i /a/wikistats/csv/ -o /a/wikistats/analytics -m $yyyymm | tee -a $log | cat

# analytics_in_page_views.csv is written daily as part of WikiCountsSummarizeProjectCounts.pl 
# part of (/home/ezachte/pageviews_monthly.sh job) 
# which processes hourly projectcounts files (per wiki page view totals for one hour) from http://dammit.lt/wikistats
# and generates several files on different aggregation levels
# only action here is to copy data to this folder to have everything in one place
# note: unlike folder name suggests this file contains stats for all projects

cp /a/wikistats/csv/csv_wp/analytics_in_page_views.csv . | tee -a $log | cat
cp /a/wikistats/csv/csv_wp/wikilytics_in_pageviews.csv . | tee -a $log | cat

zip rc-$yyyymm.zip wikilytics*.csv

